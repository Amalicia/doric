{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$cp.$                     \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.1.1`\n",
    "import $ivy.`org.typelevel::cats-core:2.3.0`\n",
    "import $ivy.`com.lihaoyi::sourcecode:0.2.6`\n",
    "import $cp.`doric_2.12-0.0.1.jar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.{col, lit}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{functions => f}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mhabla.doric._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mhabla.doric.{functions => doricf}\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions.{col, lit}\n",
    "import org.apache.spark.sql.{functions => f}\n",
    "import habla.doric._\n",
    "import habla.doric.{functions => doricf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/04/27 17:30:01 INFO SparkContext: Running Spark version 3.1.1\n",
      "21/04/27 17:30:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/04/27 17:30:01 INFO ResourceUtils: ==============================================================\n",
      "21/04/27 17:30:01 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "21/04/27 17:30:01 INFO ResourceUtils: ==============================================================\n",
      "21/04/27 17:30:01 INFO SparkContext: Submitted application: test\n",
      "21/04/27 17:30:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "21/04/27 17:30:01 INFO ResourceProfile: Limiting resource is cpu\n",
      "21/04/27 17:30:01 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "21/04/27 17:30:01 INFO SecurityManager: Changing view acls to: jovyan\n",
      "21/04/27 17:30:01 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "21/04/27 17:30:01 INFO SecurityManager: Changing view acls groups to: \n",
      "21/04/27 17:30:01 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/04/27 17:30:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "21/04/27 17:30:01 INFO Utils: Successfully started service 'sparkDriver' on port 43859.\n",
      "21/04/27 17:30:01 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/04/27 17:30:02 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/04/27 17:30:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/04/27 17:30:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/04/27 17:30:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "21/04/27 17:30:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-21926bc3-b5ac-4d91-8092-ad52c1e9f27a\n",
      "21/04/27 17:30:02 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "21/04/27 17:30:02 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/04/27 17:30:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "21/04/27 17:30:02 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://82087c81498e:4040\n",
      "21/04/27 17:30:02 INFO Executor: Starting executor ID driver on host 82087c81498e\n",
      "21/04/27 17:30:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35825.\n",
      "21/04/27 17:30:02 INFO NettyBlockTransferService: Server created on 82087c81498e:35825\n",
      "21/04/27 17:30:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/04/27 17:30:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 82087c81498e, 35825, None)\n",
      "21/04/27 17:30:02 INFO BlockManagerMasterEndpoint: Registering block manager 82087c81498e:35825 with 366.3 MiB RAM, BlockManagerId(driver, 82087c81498e, 35825, None)\n",
      "21/04/27 17:30:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 82087c81498e, 35825, None)\n",
      "21/04/27 17:30:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 82087c81498e, 35825, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@2ebf24f7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = org.apache.spark.sql.SparkSession.builder().appName(\"test\").master(\"local\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doric accelerates the spark developer with three simple steps.\n",
    "1. Column type validation\n",
    "2. Typesafety of the operations between columns\n",
    "3. Aggregation of multiple errors with improved location of the error place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Column Type validation\n",
    "\n",
    "Spark works with columns. Until runtime the dataframe doesn't know the type, but the developer knows it, and can add a validation of the expected type. Imagine that we are expecting an dataframe with an id of type string, but in our flow we can't validate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres4\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: bigint, x: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(1,10).withColumn(\"x\", f.concat(col(\"id\"), lit(\"jander\"))) // id is really a string, or spark is creating a implicit conversion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mhabla.doric.DoricMultiError: found 1 errors\nThe column with name 'id' is of type LongType and it was expected to be StringType\n\tlocated at . (cmd5.sc:3)\u001b[39m\n  habla.doric.syntax.DataFrameOps$DataframeSyntax.$anonfun$withColumn$1(\u001b[32mDataFrameOps.scala\u001b[39m:\u001b[32m26\u001b[39m)\n  cats.data.Validated.fold(\u001b[32mValidated.scala\u001b[39m:\u001b[32m29\u001b[39m)\n  habla.doric.syntax.DataFrameOps$DataframeSyntax.withColumn(\u001b[32mDataFrameOps.scala\u001b[39m:\u001b[32m29\u001b[39m)\n  ammonite.$sess.cmd5$Helper.<init>(\u001b[32mcmd5.sc\u001b[39m:\u001b[32m3\u001b[39m)\n  ammonite.$sess.cmd5$.<init>(\u001b[32mcmd5.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd5$.<clinit>(\u001b[32mcmd5.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "val df = spark.range(1,10).toDF\n",
    "\n",
    "df.withColumn(\"x\", doricf.concat(getString(\"id\"), \"jander\".lit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, doric will treat as an error that the column type is invalid, this gives doric a lot of power, only changing the way that you get a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Typesafety of the operations with columns\n",
    "\n",
    "Now that we have certinty that we will have a column of the expected type, or the program won't run, doric can prevent at compile time, errors like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdfEq\u001b[39m: \u001b[32mDataFrame\u001b[39m = [int: int, str: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfEq = List((1, \"1\"), (1, \" 1\"), (1, \" 1 \")).toDF(\"int\", \"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|int|str|  eq|\n",
      "+---+---+----+\n",
      "|  1|  1|true|\n",
      "|  1|  1|true|\n",
      "|  1| 1 |true|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfEq.withColumn(\"eq\", col(\"int\") === col(\"str\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is telling me that an integer and a string are equal?? Eaven if the strings are different?? What kind of behaviour is this??\n",
    "With doric, this code it woudn't compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd8.sc:1: The type $ScalaType cant be casted as a literal for $T.\n",
      "val res8 = dfEq.withColumn(\"eq\", getInt(\"int\") === getString(\"str\")).show\n",
      "                                               ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "dfEq.withColumn(\"eq\", getInt(\"int\") === getString(\"str\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we were wrong, and expected \"int\" column as a string, in runtime we would get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mhabla.doric.DoricMultiError: found 1 errors\nThe column with name 'int' is of type IntegerType and it was expected to be StringType\n\tlocated at . (cmd8.sc:1)\u001b[39m\n  habla.doric.syntax.DataFrameOps$DataframeSyntax.$anonfun$withColumn$1(\u001b[32mDataFrameOps.scala\u001b[39m:\u001b[32m26\u001b[39m)\n  cats.data.Validated.fold(\u001b[32mValidated.scala\u001b[39m:\u001b[32m29\u001b[39m)\n  habla.doric.syntax.DataFrameOps$DataframeSyntax.withColumn(\u001b[32mDataFrameOps.scala\u001b[39m:\u001b[32m29\u001b[39m)\n  ammonite.$sess.cmd8$Helper.<init>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd8$.<init>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd8$.<clinit>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "dfEq.withColumn(\"eq\", getString(\"int\") === getString(\"str\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning spark is hard, we don't want to also learn all the possible places that can allow us invalid code as valid, and all the magic transformations, so let us do it in our way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|int|str|   eq|\n",
      "+---+---+-----+\n",
      "|  1|  1| true|\n",
      "|  1|  1|false|\n",
      "|  1| 1 |false|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfEq.withColumn(\"eq\", getInt(\"int\").castTo[String] === getString(\"str\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|int|str|  eq|\n",
      "+---+---+----+\n",
      "|  1|  1|true|\n",
      "|  1|  1|true|\n",
      "|  1| 1 |true|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfEq.withColumn(\"eq\", getInt(\"int\") === getString(\"str\").warningCastTo[Int]).show\n",
    "// its a warning cast to because you must know that it can put a null if it can be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a accelerator for the development, we have a located point of possible error in runtime, and if they pass it, it will run as expected. And with the knowledge of what function we can apply to what column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aggregation of multiple errors with improved location of the error place\n",
    "\n",
    "Ok, we know what functions can be the reason to a runtime error, but, spark API is a fail fast API, and we have to run again and again to detect a single error, and when fixed, rerun again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdfadd\u001b[39m: \u001b[32mDataFrame\u001b[39m = [int1: int, int2: int]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfadd = List((1,2),(3,4)).toDF(\"int1\", \"int2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: cannot resolve '`Int_1`' given input columns: [int1, int2];\n'Project [int1#77, int2#78, ('Int_1 + 'Int_2) AS add#81]\n+- Project [_1#72 AS int1#77, _2#73 AS int2#78]\n   +- LocalRelation [_1#72, _2#73]\n\u001b[39m\n  org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(\u001b[32mpackage.scala\u001b[39m:\u001b[32m42\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m341\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m341\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m338\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m407\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m243\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m405\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m358\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m338\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m338\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m407\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m243\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m405\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m358\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m338\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m104\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m116\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m116\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m132\u001b[39m)\n  scala.collection.TraversableLike.$anonfun$map$1(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m285\u001b[39m)\n  scala.collection.immutable.List.foreach(\u001b[32mList.scala\u001b[39m:\u001b[32m431\u001b[39m)\n  scala.collection.TraversableLike.map(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m285\u001b[39m)\n  scala.collection.TraversableLike.map$(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m278\u001b[39m)\n  scala.collection.immutable.List.map(\u001b[32mList.scala\u001b[39m:\u001b[32m305\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m132\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m137\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m243\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m137\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m104\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m183\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m176\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m228\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m173\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(\u001b[32mQueryPlanningTracker.scala\u001b[39m:\u001b[32m111\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m143\u001b[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m772\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.executePhase(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m143\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.analyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m71\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m63\u001b[39m)\n  org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m772\u001b[39m)\n  org.apache.spark.sql.Dataset$.ofRows(\u001b[32mDataset.scala\u001b[39m:\u001b[32m88\u001b[39m)\n  org.apache.spark.sql.Dataset.withPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3715\u001b[39m)\n  org.apache.spark.sql.Dataset.select(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1462\u001b[39m)\n  org.apache.spark.sql.Dataset.withColumns(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2427\u001b[39m)\n  org.apache.spark.sql.Dataset.withColumn(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2394\u001b[39m)\n  ammonite.$sess.cmd12$Helper.<init>(\u001b[32mcmd12.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd12$.<init>(\u001b[32mcmd12.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd12$.<clinit>(\u001b[32mcmd12.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "dfadd.withColumn(\"add\", col(\"Int_1\") + col(\"Int_2\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'oh, i wrote wrong a column, let's fix it and run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: cannot resolve '`Int_2`' given input columns: [int1, int2];\n'Project [int1#77, int2#78, (int1#77 + 'Int_2) AS add#82]\n+- Project [_1#72 AS int1#77, _2#73 AS int2#78]\n   +- LocalRelation [_1#72, _2#73]\n\u001b[39m\n  org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(\u001b[32mpackage.scala\u001b[39m:\u001b[32m42\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m341\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m341\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m338\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m407\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m243\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m405\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m358\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m338\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m338\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m407\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m243\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m405\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m358\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m338\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m104\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m116\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m116\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m132\u001b[39m)\n  scala.collection.TraversableLike.$anonfun$map$1(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m285\u001b[39m)\n  scala.collection.immutable.List.foreach(\u001b[32mList.scala\u001b[39m:\u001b[32m431\u001b[39m)\n  scala.collection.TraversableLike.map(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m285\u001b[39m)\n  scala.collection.TraversableLike.map$(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m278\u001b[39m)\n  scala.collection.immutable.List.map(\u001b[32mList.scala\u001b[39m:\u001b[32m305\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m132\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m137\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m243\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m137\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m104\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m183\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m176\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m228\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m173\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(\u001b[32mQueryPlanningTracker.scala\u001b[39m:\u001b[32m111\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m143\u001b[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m772\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.executePhase(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m143\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.analyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m71\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m63\u001b[39m)\n  org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m772\u001b[39m)\n  org.apache.spark.sql.Dataset$.ofRows(\u001b[32mDataset.scala\u001b[39m:\u001b[32m88\u001b[39m)\n  org.apache.spark.sql.Dataset.withPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3715\u001b[39m)\n  org.apache.spark.sql.Dataset.select(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1462\u001b[39m)\n  org.apache.spark.sql.Dataset.withColumns(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2427\u001b[39m)\n  org.apache.spark.sql.Dataset.withColumn(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2394\u001b[39m)\n  ammonite.$sess.cmd13$Helper.<init>(\u001b[32mcmd13.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd13$.<init>(\u001b[32mcmd13.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd13$.<clinit>(\u001b[32mcmd13.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "dfadd.withColumn(\"add\", col(\"int1\") + col(\"Int_2\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(console):1:1 expected end-of-input\n",
      "next error... ok, let's fix it:\n",
      "^"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "(console):1:1 expected end-of-input\nnext error... ok, let's fix it:\n^"
     ]
    }
   ],
   "source": [
    "next error... ok, let's fix it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+\n",
      "|int1|int2|add|\n",
      "+----+----+---+\n",
      "|   1|   2|  3|\n",
      "|   3|   4|  7|\n",
      "+----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfadd.withColumn(\"add\", col(\"int1\") + col(\"int2\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doric can help us in this cases aggregating all the errors found in a dataframe transformation, in a single exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mhabla.doric.DoricMultiError: found 2 errors\nCannot resolve column name \"int_1\" among (int1, int2)\n\tlocated at . (cmd15.sc:1)\nCannot resolve column name \"int_2\" among (int1, int2)\n\tlocated at . (cmd15.sc:1)\u001b[39m\n  habla.doric.syntax.DataFrameOps$DataframeSyntax.$anonfun$withColumn$1(\u001b[32mDataFrameOps.scala\u001b[39m:\u001b[32m26\u001b[39m)\n  cats.data.Validated.fold(\u001b[32mValidated.scala\u001b[39m:\u001b[32m29\u001b[39m)\n  habla.doric.syntax.DataFrameOps$DataframeSyntax.withColumn(\u001b[32mDataFrameOps.scala\u001b[39m:\u001b[32m29\u001b[39m)\n  ammonite.$sess.cmd15$Helper.<init>(\u001b[32mcmd15.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd15$.<init>(\u001b[32mcmd15.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd15$.<clinit>(\u001b[32mcmd15.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31morg.apache.spark.sql.AnalysisException: Cannot resolve column name \"int_1\" among (int1, int2)\u001b[39m\n  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$resolveException(\u001b[32mDataset.scala\u001b[39m:\u001b[32m272\u001b[39m)\n  org.apache.spark.sql.Dataset.$anonfun$resolve$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m263\u001b[39m)\n  scala.Option.getOrElse(\u001b[32mOption.scala\u001b[39m:\u001b[32m189\u001b[39m)\n  org.apache.spark.sql.Dataset.resolve(\u001b[32mDataset.scala\u001b[39m:\u001b[32m263\u001b[39m)\n  org.apache.spark.sql.Dataset.col(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1359\u001b[39m)\n  org.apache.spark.sql.Dataset.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1326\u001b[39m)\n  habla.doric.FromDf.$anonfun$validate$1(\u001b[32mFromDf.scala\u001b[39m:\u001b[32m21\u001b[39m)\n  cats.data.KleisliApply.$anonfun$product$2(\u001b[32mKleisli.scala\u001b[39m:\u001b[32m663\u001b[39m)\n  cats.data.Kleisli.$anonfun$map$1(\u001b[32mKleisli.scala\u001b[39m:\u001b[32m40\u001b[39m)\n  habla.doric.syntax.DataFrameOps$DataframeSyntax.withColumn(\u001b[32mDataFrameOps.scala\u001b[39m:\u001b[32m23\u001b[39m)\n  ammonite.$sess.cmd15$Helper.<init>(\u001b[32mcmd15.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd15$.<init>(\u001b[32mcmd15.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd15$.<clinit>(\u001b[32mcmd15.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "dfadd.withColumn(\"add\", getInt(\"int_1\") + getInt(\"int_2\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckly my logic is very simple, and all my columns can fit in the withColumn call, but imagine that we have to split them, due to a separation of the logic and the execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: cannot resolve '`int_1`' given input columns: [int1, int2];\n'Project [int1#77, int2#78, ('int_1 + int2#78) AS add#100]\n+- Project [_1#72 AS int1#77, _2#73 AS int2#78]\n   +- LocalRelation [_1#72, _2#73]\n\u001b[39m\n  org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(\u001b[32mpackage.scala\u001b[39m:\u001b[32m42\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m341\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m341\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m338\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m407\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m243\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m405\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m358\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m338\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m338\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m407\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m243\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m405\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m358\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m338\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m104\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m116\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m116\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m132\u001b[39m)\n  scala.collection.TraversableLike.$anonfun$map$1(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m285\u001b[39m)\n  scala.collection.immutable.List.foreach(\u001b[32mList.scala\u001b[39m:\u001b[32m431\u001b[39m)\n  scala.collection.TraversableLike.map(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m285\u001b[39m)\n  scala.collection.TraversableLike.map$(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m278\u001b[39m)\n  scala.collection.immutable.List.map(\u001b[32mList.scala\u001b[39m:\u001b[32m305\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m132\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m137\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m243\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m137\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(\u001b[32mQueryPlan.scala\u001b[39m:\u001b[32m104\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(\u001b[32mTreeNode.scala\u001b[39m:\u001b[32m183\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m93\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(\u001b[32mCheckAnalysis.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m176\u001b[39m)\n  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m228\u001b[39m)\n  org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m173\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(\u001b[32mQueryPlanningTracker.scala\u001b[39m:\u001b[32m111\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m143\u001b[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m772\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.executePhase(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m143\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m73\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.analyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m71\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m63\u001b[39m)\n  org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m90\u001b[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m772\u001b[39m)\n  org.apache.spark.sql.Dataset$.ofRows(\u001b[32mDataset.scala\u001b[39m:\u001b[32m88\u001b[39m)\n  org.apache.spark.sql.Dataset.withPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3715\u001b[39m)\n  org.apache.spark.sql.Dataset.select(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1462\u001b[39m)\n  org.apache.spark.sql.Dataset.withColumns(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2427\u001b[39m)\n  org.apache.spark.sql.Dataset.withColumn(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2394\u001b[39m)\n  ammonite.$sess.cmd16$Helper.<init>(\u001b[32mcmd16.sc\u001b[39m:\u001b[32m3\u001b[39m)\n  ammonite.$sess.cmd16$.<init>(\u001b[32mcmd16.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd16$.<clinit>(\u001b[32mcmd16.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "val addColumns = col(\"int_1\") + col(\"int2\")\n",
    "\n",
    "dfadd.withColumn(\"add\", addColumns).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know its very simple to see in this logic, but the exception, marks the error in the `withColumn` method, but the real error is in other place, in the moment we asked for a unexisting column. Spark will give you a hint, but you must dive into your code to find the exact place.\n",
    "\n",
    "With doric, we can simplify it, every method that can be the reason to an error is indexed and marked in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mhabla.doric.DoricMultiError: found 1 errors\nCannot resolve column name \"int_1\" among (int1, int2)\n\tlocated at . (cmd17.sc:1)\u001b[39m\n  habla.doric.syntax.DataFrameOps$DataframeSyntax.$anonfun$withColumn$1(\u001b[32mDataFrameOps.scala\u001b[39m:\u001b[32m26\u001b[39m)\n  cats.data.Validated.fold(\u001b[32mValidated.scala\u001b[39m:\u001b[32m29\u001b[39m)\n  habla.doric.syntax.DataFrameOps$DataframeSyntax.withColumn(\u001b[32mDataFrameOps.scala\u001b[39m:\u001b[32m29\u001b[39m)\n  ammonite.$sess.cmd17$Helper.<init>(\u001b[32mcmd17.sc\u001b[39m:\u001b[32m3\u001b[39m)\n  ammonite.$sess.cmd17$.<init>(\u001b[32mcmd17.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd17$.<clinit>(\u001b[32mcmd17.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31morg.apache.spark.sql.AnalysisException: Cannot resolve column name \"int_1\" among (int1, int2)\u001b[39m\n  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$resolveException(\u001b[32mDataset.scala\u001b[39m:\u001b[32m272\u001b[39m)\n  org.apache.spark.sql.Dataset.$anonfun$resolve$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m263\u001b[39m)\n  scala.Option.getOrElse(\u001b[32mOption.scala\u001b[39m:\u001b[32m189\u001b[39m)\n  org.apache.spark.sql.Dataset.resolve(\u001b[32mDataset.scala\u001b[39m:\u001b[32m263\u001b[39m)\n  org.apache.spark.sql.Dataset.col(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1359\u001b[39m)\n  org.apache.spark.sql.Dataset.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1326\u001b[39m)\n  habla.doric.FromDf.$anonfun$validate$1(\u001b[32mFromDf.scala\u001b[39m:\u001b[32m21\u001b[39m)\n  cats.data.KleisliApply.$anonfun$product$2(\u001b[32mKleisli.scala\u001b[39m:\u001b[32m663\u001b[39m)\n  cats.data.Kleisli.$anonfun$map$1(\u001b[32mKleisli.scala\u001b[39m:\u001b[32m40\u001b[39m)\n  habla.doric.syntax.DataFrameOps$DataframeSyntax.withColumn(\u001b[32mDataFrameOps.scala\u001b[39m:\u001b[32m23\u001b[39m)\n  ammonite.$sess.cmd17$Helper.<init>(\u001b[32mcmd17.sc\u001b[39m:\u001b[32m3\u001b[39m)\n  ammonite.$sess.cmd17$.<init>(\u001b[32mcmd17.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd17$.<clinit>(\u001b[32mcmd17.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "val addColumns = getInt(\"int_1\") + getInt(\"int2\")\n",
    "\n",
    "dfadd.withColumn(\"add\", addColumns).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me copy the message of the error here again:\n",
    "\n",
    "```\n",
    "habla.doric.DoricMultiError: found 1 errors\n",
    "Cannot resolve column name \"int_1\" among (int1, int2)\n",
    "\tlocated at . (cmd25.sc:1)\n",
    "```\n",
    "\n",
    "cmd25 is the name of the file, and the line number it contains the error is number 1, if Doric could, it would fix your code, but it has to leave something for the rest. And if you are using a IDE to develop, it will create a hiperlink to the line fo the error.\n",
    "\n",
    "Lets try something \"harder\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mhabla.doric.DoricMultiError: found 2 errors\nCannot resolve column name \"int_1\" among (int1, int2)\n\tlocated at . (cmd18.sc:1)\nThe column with name 'int2' is of type IntegerType and it was expected to be StringType\n\tlocated at . (cmd18.sc:2)\u001b[39m\n  habla.doric.syntax.DataFrameOps$DataframeSyntax.$anonfun$withColumn$1(\u001b[32mDataFrameOps.scala\u001b[39m:\u001b[32m26\u001b[39m)\n  cats.data.Validated.fold(\u001b[32mValidated.scala\u001b[39m:\u001b[32m29\u001b[39m)\n  habla.doric.syntax.DataFrameOps$DataframeSyntax.withColumn(\u001b[32mDataFrameOps.scala\u001b[39m:\u001b[32m29\u001b[39m)\n  ammonite.$sess.cmd18$Helper.<init>(\u001b[32mcmd18.sc\u001b[39m:\u001b[32m5\u001b[39m)\n  ammonite.$sess.cmd18$.<init>(\u001b[32mcmd18.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd18$.<clinit>(\u001b[32mcmd18.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31morg.apache.spark.sql.AnalysisException: Cannot resolve column name \"int_1\" among (int1, int2)\u001b[39m\n  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$resolveException(\u001b[32mDataset.scala\u001b[39m:\u001b[32m272\u001b[39m)\n  org.apache.spark.sql.Dataset.$anonfun$resolve$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m263\u001b[39m)\n  scala.Option.getOrElse(\u001b[32mOption.scala\u001b[39m:\u001b[32m189\u001b[39m)\n  org.apache.spark.sql.Dataset.resolve(\u001b[32mDataset.scala\u001b[39m:\u001b[32m263\u001b[39m)\n  org.apache.spark.sql.Dataset.col(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1359\u001b[39m)\n  org.apache.spark.sql.Dataset.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1326\u001b[39m)\n  habla.doric.FromDf.$anonfun$validate$1(\u001b[32mFromDf.scala\u001b[39m:\u001b[32m21\u001b[39m)\n  cats.data.KleisliApply.$anonfun$product$2(\u001b[32mKleisli.scala\u001b[39m:\u001b[32m663\u001b[39m)\n  cats.data.Kleisli.$anonfun$map$1(\u001b[32mKleisli.scala\u001b[39m:\u001b[32m40\u001b[39m)\n  habla.doric.syntax.DataFrameOps$DataframeSyntax.withColumn(\u001b[32mDataFrameOps.scala\u001b[39m:\u001b[32m23\u001b[39m)\n  ammonite.$sess.cmd18$Helper.<init>(\u001b[32mcmd18.sc\u001b[39m:\u001b[32m5\u001b[39m)\n  ammonite.$sess.cmd18$.<init>(\u001b[32mcmd18.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd18$.<clinit>(\u001b[32mcmd18.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "val col1 = getInt(\"int_1\")\n",
    "val col2 = getString(\"int2\").warningCastTo[Int]\n",
    "val addColumns = col1 + col2\n",
    "\n",
    "dfadd.withColumn(\"add\", addColumns).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See, each error explainded and located. Right on the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, but will be asking for yourself, What is the dark side? Nothing, trully nothing, all your optimizations will work as pure Dataframe API, you still use your everydate Dataframe as normal if you want/need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives to Doric\n",
    "\n",
    "Doric is created to have a better and safer API for spark, but with the idea to be simple, and not a complete change for a spark developer.\n",
    "Spark already has a typed API in scala with DataSet, but this API is not very frendly with all the optimizations.\n",
    "\n",
    "We also have [Typelevel frameless](https://github.com/typelevel/frameless), that has a great API, that keeps the Dataset typed, and is fully compatible with the spark optimizations.\n",
    "\n",
    "But bouth DataSet and Frameless TypedDataset share a common idea, to keep the whole schema allways. In both cases it has to be recreated with a case class. For example, if we expect our Dataset with 3 columns, 2 of them as string for the name and surname, and the third as an integer for the age, we would need to model it with the following:\n",
    "\n",
    "```scala\n",
    "case class User(name: String, surname: String, age: Int)\n",
    "\n",
    "val ds: DataSet[User] = ???\n",
    "```\n",
    "If we want to enrich our row with the city, we have to use a new Structure, so we need to declare another case class\n",
    "\n",
    "```scala\n",
    "case class UserWithCity(name: String, surname: String, age: Int, city: String)\n",
    "\n",
    "val ds2: DataSet[UserWithCity] = ds.???\n",
    "```\n",
    "In both dataset and typedDataset this is required, the thing that changes is the method to call.\n",
    "\n",
    "This `case class` creation is acceptable if we don't have to transform the schema of the DataSet a lot, but if you are a spark develper, is not the normal case.\n",
    "\n",
    "Other problem of this way to work, is that we cant do functions that transform a DataSet of an inteface, i mean, if we have different Datasets, that have different schemas but share a couple of columns that we whant to transform with the same function something like:\n",
    "\n",
    "```scala\n",
    "def parseTimestamp(df: DataFrame): DataFrame = \n",
    "   df.withColumn(\"timestampParsed\", parse(df(\"timestamp\")))\n",
    "```\n",
    "\n",
    "This can be seen as an inteface:\n",
    "\n",
    "```scala\n",
    "trait WithTimestamp {\n",
    "    val timestamp: String\n",
    "}\n",
    "\n",
    "trait WithTimestampParsed {\n",
    "    val timestampParsed: Long\n",
    "}\n",
    "\n",
    "def parseTimestamp[T <: WithTimestamp](df: Dataset[T]): Dataset[T with WithTimestampParsed] =\n",
    "   df.withColumn(\"timestampParsed\", parse(df(\"timestamp\")))\n",
    "```\n",
    "\n",
    "I wish this code was as simple as i putted, but in the real world its very hard for the developer to do this.\n",
    "\n",
    "And this is the reason to create Doric, its a balance between static and type safety for columns, but keeping the DataFrame as a dynamic data structure. Allowing the developer with previous knowledge of spark to jump in almost instantly, and for somebody thats getting into spark, skip the common erros for the beginer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
